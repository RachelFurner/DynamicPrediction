{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to predict dynamic time stepping of an ocean model\n",
    "\n",
    "Written by Rachel Furner, April 2019.\n",
    "\n",
    "Collaborative work with colleagues at BAS, and the ATI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in the data\n",
    "2. Define input data and output data.\n",
    "3. Split into test and train data.\n",
    "4. Train the network\n",
    "5. Run predictions on test data and assess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from tensorflow.keras.layers import MaxPooling3D, Conv3D, UpSampling3D, Cropping3D\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepSize = 1 #number of timesteps forward which we want to predict\n",
    "SkipOver = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data files as Xarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500yr_Windx0.50\n",
      "state.0000000000.t001.nc\n",
      "['/Users/rachelfurner/JasminData/DynPred/4500yr_Windx0.50/state.0000000000.t001.nc']\n"
     ]
    }
   ],
   "source": [
    "DIR = '/Users/rachelfurner/JasminData/DynPred/'\n",
    "exp_list = ['4500yr_Windx0.50']\n",
    "file_names = ['state.0000000000.t001.nc']\n",
    "file_list =[]\n",
    "for exp in exp_list:\n",
    "    print(exp)\n",
    "    for file in file_names:\n",
    "        print(file)\n",
    "        file_list.append(os.path.join(DIR,exp,file))\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define Input output pairs\n",
    "Eventual aim is that inputs are full model field at t-stepsize and t, outputs are full model fields at t+StepSize.\n",
    "For now, input is just Temp at just at time step t, and output is Temp at t+stepsize.\n",
    "\n",
    "In future could involve lots of input variables.\n",
    "\n",
    "StepSize can be changed easily (defined further up) - plan to test different values and see how well things work.\n",
    "\n",
    "We take input,output pairs with 't' spaced by 'SkipOver' steps apart. Currently this is set as 1, but ideally would be larger, in bid to ensure some independance between samples. Balance between low values giving us lots of training samples, but also a desire for independant training samples\n",
    "\n",
    "Need to amend below to loop through multiple files, so more training data, including data from different runs can be included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 78, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (T: 2841, X: 11, Xp1: 12, Y: 78, Yp1: 79, Z: 42, Zl: 42)\n",
       "Coordinates:\n",
       "  * T        (T) float64 0.0 2.592e+06 5.184e+06 ... 7.359e+09 7.361e+09\n",
       "  * Xp1      (Xp1) float64 0.0 2.0 4.0 6.0 8.0 10.0 ... 14.0 16.0 18.0 20.0 22.0\n",
       "  * Y        (Y) float64 -59.5 -58.48 -57.44 -56.36 ... 58.48 59.5 60.5 61.5\n",
       "  * Z        (Z) float64 -5.0 -15.05 -25.25 ... -4.375e+03 -4.625e+03 -4.875e+03\n",
       "  * X        (X) float64 1.0 3.0 5.0 7.0 9.0 11.0 13.0 15.0 17.0 19.0 21.0\n",
       "  * Yp1      (Yp1) float64 -60.0 -59.0 -57.97 -56.91 ... 59.0 60.0 61.0 62.0\n",
       "  * Zl       (Zl) float64 0.0 -10.0 -20.1 -30.4 ... -4.25e+03 -4.5e+03 -4.75e+03\n",
       "Data variables:\n",
       "    iter     (T) int32 ...\n",
       "    U        (T, Z, Y, Xp1) float32 ...\n",
       "    V        (T, Z, Yp1, X) float32 ...\n",
       "    Temp     (T, Z, Y, X) float32 ...\n",
       "    S        (T, Z, Y, X) float32 ...\n",
       "    Eta      (T, Y, X) float32 ...\n",
       "    W        (T, Zl, Y, X) float32 ...\n",
       "Attributes:\n",
       "    the_run_name:    2deg\n",
       "    MITgcm_version:  checkpoint67g\n",
       "    build_user:      rfurner\n",
       "    build_host:      jasmin-sci1.ceda.ac.uk\n",
       "    build_date:      Mon Feb 18 16:24:31 GMT 2019\n",
       "    MITgcm_URL:      http://mitgcm.org\n",
       "    MITgcm_tag_id:   \n",
       "    MITgcm_mnc_ver:  0.9\n",
       "    tile_number:     1\n",
       "    bi:              1\n",
       "    bj:              1\n",
       "    sNx:             11\n",
       "    sNy:             78\n",
       "    OLx:             4\n",
       "    OLy:             4\n",
       "    nSx:             1\n",
       "    nSy:             1\n",
       "    nPx:             1\n",
       "    nPy:             1\n",
       "    Nx:              11\n",
       "    Ny:              78\n",
       "    Nr:              42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data=[]\n",
    "for file in file_list:\n",
    "    ds   = xr.open_dataset(file)\n",
    "    for time in range(StepSize, len(ds.T.data)-StepSize, SkipOver):\n",
    "        training_data.append([ds.Temp.isel(T=time-StepSize), ds.Temp.isel(T=time), ds.Temp.isel(T=time+StepSize)])\n",
    "        #training_data.append([ds.Temp.isel(T=time-StepSize), ds.Temp.isel(T=time),\n",
    "        #                      ds.S.isel   (T=time-StepSize), ds.S.isel(T=time)   ,\n",
    "        #                      ds.U.isel   (T=time-StepSize), ds.U.isel(T=time)   ,\n",
    "        #                      ds.V.isel   (T=time-StepSize), ds.V.isel(T=time)   ,\n",
    "        #                      ds.Temp.isel(T=time+StepSize),\n",
    "        #    ])\n",
    "    \n",
    "#shuffle dataset\n",
    "random.shuffle(training_data)\n",
    "print(training_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put this into X and Y arrays, ready for model to read\n",
    "\n",
    "If using multiple input variables pad with NaN's so the arrays are all the same size (not done above, as it changes them to arrays, and easier to leave as lists) - not needed when just looking at temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2839, 42, 78, 11, 2)\n",
      "(2839, 42, 78, 11)\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "#for feat1,feat2,feat3,feat4,feat5,feat6,feat7,feat8,label in training_data:\n",
    "#        X.append([\n",
    "#            np.pad(feat1.data, ((0,0),(0,1),(0,1)),'constant', constant_values=('NaN')),\n",
    "#            np.pad(feat2.data, ((0,0),(0,1),(0,1)),'constant', constant_values=('NaN')),\n",
    "#            np.pad(feat3.data, ((0,0),(0,1),(0,1)),'constant', constant_values=('NaN')),\n",
    "#            np.pad(feat4.data, ((0,0),(0,1),(0,1)),'constant', constant_values=('NaN')),\n",
    "#            np.pad(feat5.data, ((0,0),(0,1),(0,0)),'constant', constant_values=('NaN')),\n",
    "#            np.pad(feat6.data, ((0,0),(0,1),(0,0)),'constant', constant_values=('NaN')),\n",
    "#            np.pad(feat7.data, ((0,0),(0,0),(0,1)),'constant', constant_values=('NaN')),\n",
    "#            np.pad(feat8.data, ((0,0),(0,0),(0,1)),'constant', constant_values=('NaN')),\n",
    "#             ])\n",
    "#        Y.append(np.pad(label.data, ((0,0),(0,1),(0,1)),'constant', constant_values=('NaN')))\n",
    "\n",
    "for feat1,feat2,label in training_data:\n",
    "        X.append([feat1.data, feat2.data])\n",
    "        Y.append(label.data)\n",
    "\n",
    "# convert to arrays, as model wont accept a list\n",
    "X=np.array(X).transpose(0, 2, 3, 4, 1)\n",
    "Y=np.array(Y)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 78, 11, 2)\n"
     ]
    }
   ],
   "source": [
    "def normalise_data(X):\n",
    "    X=tf.keras/utils.normalize(X, axis=1)\n",
    "    return X\n",
    "\n",
    "print(X.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN Attempt 1 - an autoencoder type approach\n",
    "\n",
    "I assume that data shape is important here, and so a CNN the way to go, and have tried this with pooling layers, to get something resembling an autoencoder.  Cannot yet get the output shape to match up with the y-fields, due to the odd number of layers, and the pooling vs upscaling does not return the original shape...\n",
    "\n",
    "Also, not convinced auto-encoder approach is best option here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rachelfurner/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "alldone\n",
      "WARNING:tensorflow:From /Users/rachelfurner/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2413 samples, validate on 426 samples\n",
      "WARNING:tensorflow:From /Users/rachelfurner/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [32,44,80,12,1] vs. [32,42,78,11]\n\t [[{{node training/Adam/gradients/loss/activation_5_loss/MeanSquaredError/sub_grad/BroadcastGradientArgs}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6ba670fed2a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#fit the model - Batch size is how much to pass at once, don't want to pass all at once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#validation split - how much is train vs test data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [32,44,80,12,1] vs. [32,42,78,11]\n\t [[{{node training/Adam/gradients/loss/activation_5_loss/MeanSquaredError/sub_grad/BroadcastGradientArgs}}]]"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#first downsample/encode\n",
    "model.add(Conv3D(32, (3, 3, 3), input_shape=X.shape[1:], padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))  # output shape \n",
    "\n",
    "model.add(Conv3D(64, (3, 3, 3), padding='same'))             # output shape \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))  # output shape \n",
    "\n",
    "model.add(Conv3D(128, (3, 3, 3), padding='same'))             # output shape\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Now upsample/decode\n",
    "model.add(Conv3D(128, (3, 3, 3), padding='same'))             # output shape \n",
    "model.add(Activation('relu'))\n",
    "model.add(UpSampling3D(size=(2, 2, 2)))                       # output shape\n",
    "\n",
    "model.add(Conv3D(64, (3, 3, 3), padding='same'))              # output shape \n",
    "model.add(Activation('relu')) \n",
    "model.add(UpSampling3D(size=(2, 2, 2)))                       # output shape \n",
    "\n",
    "model.add(Conv3D(1, (3, 3, 3), padding='same'))              # output shape\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fit the model - Batch size is how much to pass at once, don't want to pass all at once. \n",
    "#validation split - how much is train vs test data.\n",
    "model.fit(X, Y, batch_size=32, epochs=3, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN attempt 2\n",
    "\n",
    "As Above fails with check, and concerns on whether I should be pooling and thus loosing some information, here I try using just conv layers - no pooling, so no longer autoencoder type set up.\n",
    "\n",
    "This runs, but results are diabolical....\n",
    "\n",
    "Number of inputs = 42*78*11*2 = 72072 inputs. This is huge..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(72072, (3, 3, 3), input_shape=X.shape[1:], padding='same'))  #no shape change with padding...\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(72072, (5, 5, 5), input_shape=X.shape[1:], padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(72072, (3, 3, 3), input_shape=X.shape[1:], padding='same'))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y, batch_size=128, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN Attemmpt 3:\n",
    "\n",
    "Also tried with Dense layers instead...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rachelfurner/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      " \n",
      " \n",
      " \n",
      "try with dense layers instead\n",
      " \n",
      "WARNING:tensorflow:From /Users/rachelfurner/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2271 samples, validate on 568 samples\n",
      "WARNING:tensorflow:From /Users/rachelfurner/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "2271/2271 [==============================] - 14s 6ms/sample - loss: 26.1384 - acc: 0.0000e+00 - val_loss: 1.3260 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "2271/2271 [==============================] - 2s 911us/sample - loss: 0.5775 - acc: 0.0000e+00 - val_loss: 0.1475 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "2271/2271 [==============================] - 1s 585us/sample - loss: 0.0764 - acc: 0.0000e+00 - val_loss: 0.0280 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xd4ff95978>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#'flatten' inputs\n",
    "model.add(Flatten())   # should be dim 960\n",
    "# define first hidden layer\n",
    "model.add(Dense(72072))\n",
    "model.add(Activation('relu'))\n",
    "#add second hidden layer \n",
    "model.add(Dense(72072))\n",
    "model.add(Activation('relu'))\n",
    "#add third layer - output payer\n",
    "model.add(Dense(72072))\n",
    "#ouput layer should have linear activation function suitable for a regression problem\n",
    "model.add(Activation('linear'))\n",
    "#reshape\n",
    "model.add(Reshape(Y.shape[1:]))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y, batch_size=128, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN Attempt 4\n",
    "\n",
    "A combination of Conv and dense layers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(72072, (3, 3,3), input_shape=X.shape[1:], padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(72072, (5, 5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(72072, (7, 7, 7), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(72072))\n",
    "#last layer should have linear activation function suitable for a regression problem\n",
    "model.add(Activation('linear'))\n",
    "#reshape\n",
    "model.add(Reshape(Y.shape[1:]))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y, batch_size=128, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
