{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hpcdata/users/racfur/conda-envs/RF_hpc_clean/lib/python3.7/site-packages/xarray/core/merge.py:17: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report, r2_score\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepSize = 1 # how many output steps (months!) to predict over\n",
    "\n",
    "halo_size = 1\n",
    "halo_list = (range(-halo_size, halo_size+1))\n",
    "#Calculate x,y position in each feature list to use as 'now' value\n",
    "xy=2*(halo_size^2+halo_size) # if using a 2-d halo in x and y....if using a 3-d halo need to recalculate!\n",
    "\n",
    "# region at start of run to learn from\n",
    "split_index_yr=200\n",
    "split_index = 12*split_index_yr   # learn from first 200 years\n",
    "\n",
    "my_var = 'Ttave'\n",
    "#my_var = 'uVeltave'\n",
    "\n",
    "#my_var = 'Stave'\n",
    "#my_var = 'uVeltave'\n",
    "\n",
    "exp_name=my_var+'_tr'+str(split_index_yr)+'_halo'+str(halo_size)+'_pred'+str(StepSize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hpcdata/users/racfur/MITGCM_OUTPUT/20000yr_Windx1.00_mm_diag/cat_tave_selectvars_5000yrs.nc\n"
     ]
    }
   ],
   "source": [
    "DIR = '/data/hpcdata/users/racfur/MITGCM_OUTPUT/'\n",
    "exp_list = ['20000yr_Windx1.00_mm_diag/']\n",
    "file_list =[]\n",
    "#for exp in exp_list:\n",
    "#    for filename in os.listdir(os.path.join(DIR,exp)):\n",
    "#        if filename.__contains__('cat_tave_selectvars.nc'):\n",
    "#            file_list.append(os.path.join(DIR,exp,filename))\n",
    "#ds   = xr.open_dataset(file_list[0])\n",
    "\n",
    "filename='/data/hpcdata/users/racfur/MITGCM_OUTPUT/20000yr_Windx1.00_mm_diag/cat_tave_selectvars_5000yrs.nc'\n",
    "print(filename)\n",
    "ds   = xr.open_dataset(filename)\n",
    "\n",
    "ds_var=ds[my_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot time series of input (and output) data for a random point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot a timeseries of variable value at set grid point and its halo, \n",
    "# # to use this to predict the value of that variable at a later time step\n",
    "\n",
    "# z=np.random.randint(halo_size,41-halo_size) # 42 points in z-dir - skip edge points, don't want to be near surface or bottom boundary *for now*.\n",
    "# x=np.random.randint(halo_size,10-1-halo_size)  # 11 points in x dir - skip edge points to allow non-boundary halo - skip extra point as its land\n",
    "# y=np.random.randint(halo_size,77-1-halo_size)  # 78 points in y dir - skip edge points to allow non-boundary halo - skip extra point as its land\n",
    "\n",
    "# colours=['blue' , 'green', 'red', 'cyan', 'magenta', 'yellow', 'orange', 'purple', 'aquamarine',\n",
    "#          'gold', 'brown', 'pink', 'grey', 'olive', 'yellowgreen', 'violet', 'teal', 'sienna',\n",
    "#          'salmon', 'plum', 'navy', 'orangered', 'tan', 'lightblue', 'khaki', 'indigo', \n",
    "#          'darkgreen', 'crimson']\n",
    "# colours.insert(xy, 'black')\n",
    "# styles=['dotted'] * 28\n",
    "# styles.insert(xy, 'solid')\n",
    "# alphas=[0.4] * 28\n",
    "# alphas.insert(xy, 1.)\n",
    "# thicks=['0.5'] * 28\n",
    "# thicks.insert(xy, '2')\n",
    "\n",
    "# fig1 = plt.figure(figsize=(18,4))\n",
    "# i=0\n",
    "# for x_offset in halo_list:\n",
    "#     for y_offset in halo_list:\n",
    "#         ds_var[:split_index,z,y+y_offset,x+x_offset].plot(label=('x+%d,y+%d' % (x_offset, y_offset)), \n",
    "#                                                           alpha=1.0, xscale='log', color=colours[i], lw=thicks[i])\n",
    "#         ds_var[split_index:,z,y+y_offset,x+x_offset].plot(alpha=0.4, xscale='log', color=colours[i], lw=thicks[i])\n",
    "#         plt.xlabel('model time [s] (log scale)')\n",
    "#         i=i+1\n",
    "\n",
    "# fig1.legend(loc='center right')\n",
    "# fig1.savefig('../regression_plots/'+exp_name+'_'+str(x)+'.'+str(y)+'.'+str(z)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data as training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3292bcb903b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m74\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0minputs_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my_offset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx_offset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_offset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhalo_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_offset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhalo_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mnxt_outputs_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mStepSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3292bcb903b3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m74\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0minputs_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my_offset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx_offset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_offset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhalo_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_offset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhalo_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mnxt_outputs_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mStepSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_var' is not defined"
     ]
    }
   ],
   "source": [
    "inputs_tr = []\n",
    "nxt_outputs_tr = []\n",
    "\n",
    "inputs_te = []\n",
    "nxt_outputs_te = []\n",
    "\n",
    "# Read in as training and test data (rather than reading in all data and splitting),\n",
    "# so we can learn on first n time steps, and test on rest\n",
    "\n",
    "for z in range(1,40,10):\n",
    "    for x in range(2,7,3):\n",
    "        for y in range(2,74,10):\n",
    "            for time in range(0, split_index, 20):\n",
    "                inputs_tr.append([ds_var.isel(T=time)[z,y+y_offset,x+x_offset] for x_offset in halo_list for y_offset in halo_list])\n",
    "                nxt_outputs_tr.append([ds_var.isel(T=time+StepSize)[z,y,x]])\n",
    "                \n",
    "            for time in range(split_index, len(ds.T.data)-StepSize, 200):\n",
    "                inputs_te.append([ds_var.isel(T=time)[z,y+y_offset,x+x_offset] for x_offset in halo_list for y_offset in halo_list])\n",
    "                nxt_outputs_te.append([ds_var.isel(T=time+StepSize)[z,y,x]])\n",
    "                \n",
    "inputs_tr=np.asarray(inputs_tr)\n",
    "nxt_outputs_tr=np.asarray(nxt_outputs_tr)\n",
    "\n",
    "inputs_te=np.asarray(inputs_te)\n",
    "nxt_outputs_te=np.asarray(nxt_outputs_te)\n",
    "\n",
    "##calculate outputs for tendancy model as the difference between 'next' and 'now'\n",
    "tnd_outputs_tr = nxt_outputs_tr[:,0] - inputs_tr[:,xy]\n",
    "tnd_outputs_te = nxt_outputs_te[:,0] - inputs_te[:,xy]\n",
    "\n",
    "print(inputs_tr.shape, nxt_outputs_tr.shape, tnd_outputs_tr.shape)\n",
    "print(inputs_te.shape, nxt_outputs_te.shape, tnd_outputs_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data by outputting and plotting a few random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output some random samples of test and train and plot\n",
    "\n",
    "#sample_no=np.random.randint(0,inputs_tr.shape[0])\n",
    "#print(sample_no)\n",
    "sample_no=40\n",
    "print(inputs_tr[sample_no,:])\n",
    "print(inputs_tr[sample_no,xy])\n",
    "print(nxt_outputs_tr[sample_no])\n",
    "print(tnd_outputs_tr[sample_no])\n",
    "\n",
    "features = range(inputs_tr.shape[1])\n",
    "fig = plt.figure()\n",
    "ax=plt.subplot()\n",
    "for feature in features:\n",
    "    plt.scatter(1,inputs_tr[sample_no,feature])\n",
    "plt.scatter(2,nxt_outputs_tr[sample_no])\n",
    "#plt.scatter(3,tnd_outputs_tr[sample_no])\n",
    "ax.set_xlim(0,4)\n",
    "\n",
    "#sample_no=np.random.randint(0,inputs_te.shape[0])\n",
    "print('')\n",
    "#print(sample_no)\n",
    "sample_no=100\n",
    "print(inputs_te[sample_no,:])\n",
    "print(inputs_te[sample_no,xy])\n",
    "print(nxt_outputs_te[sample_no])\n",
    "print(tnd_outputs_te[sample_no])\n",
    "\n",
    "features = range(inputs_te.shape[1])\n",
    "fig = plt.figure()\n",
    "ax=plt.subplot()\n",
    "for feature in features:\n",
    "    plt.scatter(1,inputs_te[sample_no,feature])\n",
    "plt.scatter(2,nxt_outputs_te[sample_no])\n",
    "#plt.scatter(3,tnd_outputs_te[sample_no])\n",
    "ax.set_xlim(0,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalise (based on training data only)\n",
    "def normalise_data(train,test):\n",
    "    train_mean, train_std = np.mean(train), np.std(train)\n",
    "    norm_train = (train - train_mean) / train_std\n",
    "    norm_test  = (test - train_mean) / train_std\n",
    "    return norm_train, norm_test\n",
    "\n",
    "# normalise inputs\n",
    "norm_inputs_tr = np.zeros(inputs_tr.shape)\n",
    "norm_inputs_te = np.zeros(inputs_te.shape)\n",
    "\n",
    "for i in range(inputs_tr.shape[1]):  #loop over each feature, normalising individually\n",
    "    norm_inputs_tr[:, i], norm_inputs_te[:, i] = normalise_data(inputs_tr[:, i], inputs_te[:, i])\n",
    "\n",
    "#normalise nxt and tnd outputs\n",
    "norm_nxt_outputs_tr, norm_nxt_outputs_te = normalise_data(nxt_outputs_tr[:], nxt_outputs_te[:])\n",
    "norm_tnd_outputs_tr, norm_tnd_outputs_te = normalise_data(tnd_outputs_tr[:], tnd_outputs_te[:])\n",
    "\n",
    "# Calc mean and std of outputs re-forming predictions\n",
    "nxt_outputs_tr_mean = np.mean(nxt_outputs_tr)\n",
    "nxt_outputs_tr_std = np.std(nxt_outputs_tr)\n",
    "tnd_outputs_tr_mean = np.mean(tnd_outputs_tr)\n",
    "tnd_outputs_tr_std  = np.std(tnd_outputs_tr)\n",
    "print(np.mean(nxt_outputs_tr))\n",
    "print(np.std(nxt_outputs_tr))\n",
    "print(np.mean(tnd_outputs_tr))\n",
    "print(np.std(tnd_outputs_tr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a model to directly predict variable value at next time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate and plot 'persistance' score, to give a baseline\n",
    "\n",
    "predict_persistance_nxt = norm_inputs_te[:,xy]\n",
    "print(predict_persistance_nxt.shape)\n",
    "\n",
    "pers_nxt_r2 = r2_score(norm_nxt_outputs_te, predict_persistance_nxt)\n",
    "pers_nxt_maxer = metrics.max_error(norm_nxt_outputs_te, predict_persistance_nxt)\n",
    "pers_nxt_mse = metrics.mean_squared_error(norm_nxt_outputs_te, predict_persistance_nxt)\n",
    "\n",
    "print('persistance r2 score ; ', pers_nxt_r2)\n",
    "print('persistance max error ; ', pers_nxt_maxer)\n",
    "print('persistance mean squared error ; ', pers_nxt_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune alpha using cross validation, and neg mean squared error\n",
    "\n",
    "alpha_s = [0.00, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30]\n",
    "\n",
    "parameters = [{'alpha': alpha_s}]\n",
    "n_folds=5\n",
    "scoring={'max_error', 'neg_mean_squared_error', 'r2'}\n",
    "\n",
    "model_nxt=linear_model.Ridge()\n",
    "\n",
    "# Calculate training scores using cross validation with various values of alpha\n",
    "model_nxt_cv = GridSearchCV(model_nxt, parameters, cv=n_folds, scoring=scoring, refit='neg_mean_squared_error')\n",
    "model_nxt_cv.fit(norm_inputs_tr, norm_nxt_outputs_tr)\n",
    "results = model_nxt_cv.cv_results_\n",
    "\n",
    "best_params=model_nxt_cv.best_params_\n",
    "best_alpha = (best_params['alpha'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For neg mean squared error, calculate and plot the mean score, and the region +/- one s.d\n",
    "neg_mean_squared_error_mean = model_nxt_cv.cv_results_['mean_test_neg_mean_squared_error']\n",
    "neg_mean_squared_error_std = model_nxt_cv.cv_results_['std_test_neg_mean_squared_error']\n",
    "neg_mean_squared_error_std_error = neg_mean_squared_error_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alpha_s, neg_mean_squared_error_mean, label='mean neg-mean-squared-error from CV', color='blue')\n",
    "plt.fill_between(alpha_s, neg_mean_squared_error_mean + neg_mean_squared_error_std_error,\n",
    "                 neg_mean_squared_error_mean - neg_mean_squared_error_std_error, alpha=0.1, color='blue')\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "#plt.axhline(np.max(r2_scores), linestyle='--', color='.5')\n",
    "# Also calculate training and validation scores for same values of alpha without CV \n",
    "# and overplot - seems there is something wrong here....\n",
    "# The training scores should be similar... but are way out!\n",
    "val_scores = []\n",
    "train_scores = []\n",
    "for alpha in alpha_s:\n",
    "    model_nxt.set_params(alpha=alpha)\n",
    "    model_nxt.fit(norm_inputs_tr, norm_nxt_outputs_tr)\n",
    "    train_scores.append(- metrics.mean_squared_error(norm_nxt_outputs_tr, model_nxt.predict(norm_inputs_tr)) )\n",
    "    val_scores.append(- metrics.mean_squared_error(norm_nxt_outputs_te, model_nxt.predict(norm_inputs_te)) )\n",
    "plt.plot(alpha_s, train_scores, label='training score')\n",
    "plt.plot(alpha_s, val_scores, label='validation score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For r2 score, calculate and plot the mean score, and the region +/- one s.d\n",
    "\n",
    "r2_mean = model_nxt_cv.cv_results_['mean_test_r2']\n",
    "r2_std = model_nxt_cv.cv_results_['std_test_r2']\n",
    "r2_std_error = r2_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alpha_s, r2_mean, label='mean r2 score from CV', color='blue')\n",
    "plt.fill_between(alpha_s, r2_mean + r2_std_error, r2_mean - r2_std_error, alpha=0.1, color='blue')\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "#plt.axhline(np.max(r2_scores), linestyle='--', color='.5')\n",
    "# Also calculate training and validation scores for same values of alpha without CV \n",
    "# and overplot - seems there is something wrong here....\n",
    "# The training scores should be similar... but are way out!\n",
    "val_scores = []\n",
    "train_scores = []\n",
    "for alpha in alpha_s:\n",
    "    model_nxt.set_params(alpha=alpha)\n",
    "    model_nxt.fit(norm_inputs_tr, norm_nxt_outputs_tr)\n",
    "    train_scores.append(model_nxt.score(norm_inputs_tr, norm_nxt_outputs_tr))\n",
    "    val_scores.append(model_nxt.score(norm_inputs_te, norm_nxt_outputs_te))#\n",
    "plt.plot(alpha_s, train_scores, label='training score')\n",
    "plt.plot(alpha_s, val_scores, label='validation score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Max Error, plot the mean score, and the region +/- one s.d, (huge s.d. as you'd expect)\n",
    "max_error_mean = model_nxt_cv.cv_results_['mean_test_max_error']\n",
    "max_error_std = model_nxt_cv.cv_results_['std_test_max_error']\n",
    "max_error_std_error = max_error_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alpha_s, max_error_mean, label='mean negative-max-error from CV', color='red')\n",
    "plt.fill_between(alpha_s, max_error_mean + max_error_std_error, max_error_mean - max_error_std_error, alpha=0.1, color='red')\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "# Also calculate training and validation scores for same values of alpha without CV \n",
    "# and overplot - seems there is something wrong here....\n",
    "# The training scores should be similar... but are way out!\n",
    "val_scores = []\n",
    "train_scores = []\n",
    "for alpha in alpha_s:\n",
    "    model_nxt.set_params(alpha=alpha)\n",
    "    model_nxt.fit(norm_inputs_tr, norm_nxt_outputs_tr)\n",
    "    train_scores.append(- metrics.max_error(norm_nxt_outputs_tr, model_nxt.predict(norm_inputs_tr)) )\n",
    "    val_scores.append(- metrics.max_error(norm_nxt_outputs_te, model_nxt.predict(norm_inputs_te)) )\n",
    "plt.plot(alpha_s, train_scores, label='training score')\n",
    "plt.plot(alpha_s, val_scores, label='validation score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having Tuned Alpha set up and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model, predict next time step value, and assess, having selected a best alpha from above using mean-square scores\n",
    "\n",
    "model_nxt= linear_model.Ridge(alpha=best_alpha)\n",
    "# eventually might want to aim for a zero intercept, but model seems to struggle a lot with this so leave for now\n",
    "\n",
    "model_nxt.fit(norm_inputs_tr, norm_nxt_outputs_tr )  # train to evaluate the value at the next time step...\n",
    "\n",
    "print('coefs     : ' + str(model_nxt.coef_))\n",
    "print('intercept : ' + str(model_nxt.intercept_))\n",
    "print(' ')\n",
    "\n",
    "predicted_nxt = model_nxt.predict(norm_inputs_te)\n",
    "\n",
    "nxt_r2 = r2_score(norm_nxt_outputs_te, predicted_nxt)\n",
    "nxt_maxer = metrics.max_error(norm_nxt_outputs_te, predicted_nxt)\n",
    "nxt_mse = metrics.mean_squared_error(norm_nxt_outputs_te, predicted_nxt)\n",
    "\n",
    "print('persistance r2 score ; ', pers_nxt_r2)\n",
    "print('persistance max error ; ', pers_nxt_maxer)\n",
    "print('persistance mean squared error ; ', pers_nxt_mse)\n",
    "print('')\n",
    "print('model r2 score ; ', nxt_r2)\n",
    "print('model max error ; ', nxt_maxer)\n",
    "print('model mean squared error ; ', nxt_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first plot normalised prediction against truth\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.scatter(norm_nxt_outputs_te, predicted_nxt, edgecolors=(0, 0, 0))\n",
    "ax.plot([norm_nxt_outputs_te.min(), norm_nxt_outputs_te.max()], [norm_nxt_outputs_te.min(), norm_nxt_outputs_te.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Truth')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title('Normalised Predicted values against GCM values\\nnetwork tuned to predict value of '+my_var+' at next time step')\n",
    "ax.text(.05,.9,'Mean Squared Error: {:.4e}'.format(nxt_mse),transform=ax.transAxes)\n",
    "plt.savefig('../regression_plots/'+exp_name+'_predictedVtruth_nxt.png')\n",
    "plt.show()\n",
    "\n",
    "#de-normalise predicted values and plot against truth\n",
    "\n",
    "denorm_predicted_nxt = predicted_nxt*nxt_outputs_tr_std+nxt_outputs_tr_mean\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.scatter(nxt_outputs_te, denorm_predicted_nxt, edgecolors=(0, 0, 0))\n",
    "ax.plot([nxt_outputs_te.min(), nxt_outputs_te.max()], [nxt_outputs_te.min(), nxt_outputs_te.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Truth')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title('Predicted values against GCM values\\nnetwork tuned to predict value of '+my_var+' at next time step')\n",
    "ax.text(.05,.9,'Mean Squared Error: {:.4e}'.format(nxt_mse),transform=ax.transAxes)\n",
    "plt.savefig('../regression_plots/'+exp_name+'_predictedVtruth_nxt.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new model to predict tendancy (difference between now and next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate and plot 'persistance' score, to give a baseline\n",
    "\n",
    "predict_persistance_tnd = np.zeros([tnd_outputs_te.shape[0]])\n",
    "\n",
    "pers_tnd_r2 = r2_score(norm_tnd_outputs_te, predict_persistance_tnd)\n",
    "pers_tnd_maxer = metrics.max_error(norm_tnd_outputs_te, predict_persistance_tnd)\n",
    "pers_tnd_mse = metrics.mean_squared_error(norm_tnd_outputs_te, predict_persistance_tnd)\n",
    "\n",
    "print('persistance r2 score ; ', pers_tnd_r2)\n",
    "print('persistance max error ; ', pers_tnd_maxer)\n",
    "print('persistance mean squared error ; ', pers_tnd_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune alpha using cross validation, and neg mean squared error\n",
    "\n",
    "alpha_s = [0.00, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30]\n",
    "\n",
    "parameters = [{'alpha': alpha_s}]\n",
    "n_folds=5\n",
    "scoring={'max_error', 'neg_mean_squared_error', 'r2'}\n",
    "\n",
    "model_tnd=linear_model.Ridge()\n",
    "\n",
    "# Calculate training scores using cross validation with various values of alpha\n",
    "model_tnd_cv = GridSearchCV(model_tnd, parameters, cv=n_folds, scoring=scoring, refit='neg_mean_squared_error')\n",
    "model_tnd_cv.fit(norm_inputs_tr, norm_tnd_outputs_tr)\n",
    "results = model_tnd_cv.cv_results_\n",
    "\n",
    "best_params=model_tnd_cv.best_params_\n",
    "best_alpha = (best_params['alpha'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For neg mean squared error, calculate and plot the mean score, and the region +/- one s.d\n",
    "neg_mean_squared_error_mean = model_tnd_cv.cv_results_['mean_test_neg_mean_squared_error']\n",
    "neg_mean_squared_error_std = model_tnd_cv.cv_results_['std_test_neg_mean_squared_error']\n",
    "neg_mean_squared_error_std_error = neg_mean_squared_error_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alpha_s, neg_mean_squared_error_mean, label='mean neg-mean-squared-error from CV', color='blue')\n",
    "plt.fill_between(alpha_s, neg_mean_squared_error_mean + neg_mean_squared_error_std_error,\n",
    "                 neg_mean_squared_error_mean - neg_mean_squared_error_std_error, alpha=0.1, color='blue')\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "#plt.axhline(np.max(r2_scores), linestyle='--', color='.5')\n",
    "\n",
    "# Also calculate training and validation scores for same values of alpha without CV \n",
    "# and overplot - seems there is something wrong here....\n",
    "# The training scores should be similar... but are way out!\n",
    "val_scores = []\n",
    "train_scores = []\n",
    "for alpha in alpha_s:\n",
    "    model_tnd.set_params(alpha=alpha)\n",
    "    model_tnd.fit(norm_inputs_tr, norm_tnd_outputs_tr)\n",
    "    train_scores.append(- metrics.mean_squared_error(norm_tnd_outputs_tr, model_tnd.predict(norm_inputs_tr)) )\n",
    "    val_scores.append(- metrics.mean_squared_error(norm_tnd_outputs_te, model_tnd.predict(norm_inputs_te)) )\n",
    "plt.plot(alpha_s, train_scores, label='training score')\n",
    "plt.plot(alpha_s, val_scores, label='validation score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For r2 score, calculate and plot the mean score, and the region +/- one s.d\n",
    "\n",
    "r2_mean = model_tnd_cv.cv_results_['mean_test_r2']\n",
    "r2_std = model_tnd_cv.cv_results_['std_test_r2']\n",
    "r2_std_error = r2_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alpha_s, r2_mean, label='mean r2 score from CV', color='blue')\n",
    "plt.fill_between(alpha_s, r2_mean + r2_std_error, r2_mean - r2_std_error, alpha=0.1, color='blue')\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "#plt.axhline(np.max(r2_scores), linestyle='--', color='.5')\n",
    "# Also calculate training and validation scores for same values of alpha without CV \n",
    "# and overplot - seems there is something wrong here....\n",
    "# The training scores should be similar... but are way out!\n",
    "val_scores = []\n",
    "train_scores = []\n",
    "for alpha in alpha_s:\n",
    "    model_tnd.set_params(alpha=alpha)\n",
    "    model_tnd.fit(norm_inputs_tr, norm_tnd_outputs_tr)\n",
    "    train_scores.append(model_tnd.score(norm_inputs_tr, norm_tnd_outputs_tr))\n",
    "    val_scores.append(model_tnd.score(norm_inputs_te, norm_tnd_outputs_te))#\n",
    "plt.plot(alpha_s, train_scores, label='training score')\n",
    "plt.plot(alpha_s, val_scores, label='validation score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Max Error, plot the mean score, and the region +/- one s.d, (huge s.d. as you'd expect)\n",
    "max_error_mean = model_tnd_cv.cv_results_['mean_test_max_error']\n",
    "max_error_std = model_tnd_cv.cv_results_['std_test_max_error']\n",
    "max_error_std_error = max_error_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alpha_s, max_error_mean, label='mean negative-max-error from CV', color='red')\n",
    "plt.fill_between(alpha_s, max_error_mean + max_error_std_error, max_error_mean - max_error_std_error, alpha=0.1, color='red')\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "# Also calculate training and validation scores for same values of alpha without CV \n",
    "# and overplot - seems there is something wrong here....\n",
    "# The training scores should be similar... but are way out!\n",
    "val_scores = []\n",
    "train_scores = []\n",
    "for alpha in alpha_s:\n",
    "    model_tnd.set_params(alpha=alpha)\n",
    "    model_tnd.fit(norm_inputs_tr, norm_tnd_outputs_tr)\n",
    "    train_scores.append(- metrics.max_error(norm_tnd_outputs_tr, model_tnd.predict(norm_inputs_tr)) )\n",
    "    val_scores.append(- metrics.max_error(norm_tnd_outputs_te, model_tnd.predict(norm_inputs_te)) )\n",
    "plt.plot(alpha_s, train_scores, label='training score')\n",
    "plt.plot(alpha_s, val_scores, label='validation score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having Tuned Alpha set up and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model to predict next time step value, having selected a best alpha from above, using r2 scores\n",
    "\n",
    "model_tnd= linear_model.Ridge(alpha=best_alpha, normalize=False)\n",
    "# eventually might want to aim for a zero intercept, but model seems to struggle a lot with this so leave for now\n",
    "\n",
    "model_tnd.fit(norm_inputs_tr, norm_tnd_outputs_tr )  # train to evaluate the value at the next time step...\n",
    "\n",
    "print('coefs     : ' + str(model_tnd.coef_))\n",
    "print('intercept : ' + str(model_tnd.intercept_))\n",
    "print(' ')\n",
    "\n",
    "predicted_tnd = model_tnd.predict(norm_inputs_te)\n",
    "tnd_r2 = r2_score(norm_tnd_outputs_te, predicted_tnd)\n",
    "tnd_maxer = metrics.max_error(norm_tnd_outputs_te, predicted_tnd)\n",
    "tnd_mse = metrics.mean_squared_error(norm_tnd_outputs_te, predicted_tnd)\n",
    "\n",
    "\n",
    "print('persistance r2 score ; ', r2_score(norm_tnd_outputs_te, predict_persistance_tnd))\n",
    "print('persistance max error ; ', metrics.max_error(norm_tnd_outputs_te, predict_persistance_tnd))\n",
    "print('persistance mean squared error ; ', metrics.mean_squared_error(norm_tnd_outputs_te, predict_persistance_tnd))\n",
    "print('')\n",
    "print('model r2 score ; ', tnd_r2)\n",
    "print('model max error ; ', tnd_maxer)\n",
    "print('model mean squared error ; ', tnd_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plot normalised values\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.scatter(norm_tnd_outputs_te, predicted_tnd, edgecolors=(0, 0, 0))\n",
    "ax.plot([norm_tnd_outputs_te.min(), norm_tnd_outputs_te.max()], [norm_tnd_outputs_te.min(), norm_tnd_outputs_te.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Truth')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title('Normalised predicted values against GCM values\\nnetwork tuned to predict change in '+my_var+' over next time step')\n",
    "ax.text(.05,.9,'Mean Squared Error: {:.4e}'.format(tnd_mse),transform=ax.transAxes)\n",
    "plt.savefig('../regression_plots/'+exp_name+'_predictedVtruth_tnd.png')\n",
    "plt.show()\n",
    "\n",
    "#de-normalise predicted values and plot against truth\n",
    "\n",
    "denorm_predicted_tnd = predicted_tnd*tnd_outputs_tr_std+tnd_outputs_tr_mean\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.scatter(tnd_outputs_te, denorm_predicted_tnd, edgecolors=(0, 0, 0))\n",
    "ax.plot([tnd_outputs_te.min(), tnd_outputs_te.max()], [tnd_outputs_te.min(), tnd_outputs_te.max()], 'k--', lw=1)\n",
    "ax.set_xlabel('Truth')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title('Predicted values against GCM values\\nnetwork tuned to predict change in '+my_var+' over next time step')\n",
    "ax.text(.05,.9,'Mean Squared Error: {:.4e}'.format(tnd_mse),transform=ax.transAxes)\n",
    "if my_var=='uVeltave':\n",
    "    plt.xlim(-0.001,0.001)\n",
    "    plt.ylim(-0.001,0.001)\n",
    "plt.savefig('../regression_plots/'+exp_name+'_predictedVtruth_tnd.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all scores in one place, and to file:\n",
    "\n",
    "print('next persistance r2 score ; ', pers_nxt_r2)\n",
    "print('next persistance max error ; ', pers_nxt_maxer)\n",
    "print('next persistance mean squared error ; ', pers_nxt_mse)\n",
    "print('')\n",
    "print('next model r2 score ; ', nxt_r2)\n",
    "print('next model max error ; ', nxt_maxer)\n",
    "print('next model mean squared error ; ', nxt_mse)\n",
    "print('')\n",
    "print('tend persistance r2 score ; ', r2_score(norm_tnd_outputs_te, predict_persistance_tnd))\n",
    "print('tend persistance max error ; ', metrics.max_error(norm_tnd_outputs_te, predict_persistance_tnd))\n",
    "print('tend persistance mean squared error ; ', metrics.mean_squared_error(norm_tnd_outputs_te, predict_persistance_tnd))\n",
    "print('')\n",
    "print('tend model r2 score ; ', tnd_r2)\n",
    "print('tend model max error ; ', tnd_maxer)\n",
    "print('tend model mean squared error ; ', tnd_mse)\n",
    "\n",
    "file=open('../regression_plots/'+exp_name+'.txt',\"w+\")\n",
    "file.write('mean_nxtoutputs %.10f; \\n' % np.mean(nxt_outputs_tr))\n",
    "file.write('std_nxtoutputs .10f; \\n' % np.std(nxt_outputs_tr))\n",
    "file.write('mean_tndoutputs .10f; \\n' % np.mean(tnd_outputs_tr))\n",
    "file.write('std_tndoutputs .10f; \\n' % np.std(tnd_outputs_tr))\n",
    "file.write('\\n')\n",
    "file.write('next persistance r2 score %.10f; \\n' % pers_nxt_r2)\n",
    "file.write('next persistance max error %.4f; \\n' % pers_nxt_maxer)\n",
    "file.write('next persistance mean squared error %.4e; \\n' % pers_nxt_mse)\n",
    "file.write('next persistance rms error %.4e; \\n' % np.sqrt(pers_nxt_mse))\n",
    "file.write('\\n')\n",
    "file.write('next model r2 score %.10f; \\n' %  nxt_r2)\n",
    "file.write('next model max error %.4f; \\n' %  nxt_maxer)\n",
    "file.write('next model mean squared error %.4e; \\n' % nxt_mse)\n",
    "file.write('next model rms error %.4e; \\n' % np.sqrt(nxt_mse))\n",
    "file.write('\\n')\n",
    "file.write('tend persistance r2 score %.10f; \\n' % pers_tnd_r2)\n",
    "file.write('tend persistance max error %.4f; \\n' % pers_tnd_maxer)\n",
    "file.write('tend persistance mean squared error %.4e; \\n' % pers_tnd_mse)\n",
    "file.write('tend persistance rms error %.4E; \\n' % np.sqrt(pers_tnd_mse))\n",
    "file.write('\\n')\n",
    "file.write('tend model r2 score %.10f; \\n' % tnd_r2)\n",
    "file.write('tend model max error %.4f; \\n' % tnd_maxer)\n",
    "file.write('tend model mean squared error %.4e; \\n' % tnd_mse)\n",
    "file.write('tend model rms error %.4e; \\n' % np.sqrt(tnd_mse))\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempts to bug fix code - something is wrong as test data always performs better than training data....?!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Results are suspicious... validation score should be worse than test, and should increase with alpha...\n",
    "# # possible its due to the organised separation of test and train... investigate by combining train \n",
    "# # and test data and then randomly separating and running...\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# inputs = np.vstack((norm_inputs_tr, norm_inputs_te))\n",
    "# outputs = np.vstack((norm_nxt_outputs_tr, norm_nxt_outputs_te))\n",
    "# data = np.hstack((inputs, outputs))\n",
    "# np.random.shuffle(data)\n",
    "\n",
    "# #Split data into two sets\n",
    "# in_set1, in_set2, out_set1, out_set2 = train_test_split(data[:,0:8], data[:,9:] , test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_s = [0.00, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30]\n",
    "\n",
    "# #Run with set1 used as training data\n",
    "# set1_test1 = []\n",
    "# set2_test1 = []\n",
    "# for alpha in alpha_s:\n",
    "#     model_test1 = linear_model.Ridge(alpha=alpha)\n",
    "#     # eventually might want to aim for a zero intercept, but model seems to struggle a lot with this so leave for now\n",
    "#     # Fit model based on training data (and alpha)\n",
    "#     model_test1.fit(in_set1, out_set1)\n",
    "#     set1_test1.append(metrics.r2_score(model_test1.predict(in_set1), out_set1))\n",
    "#     set2_test1.append(metrics.r2_score(model_test1.predict(in_set2), out_set2))\n",
    "#     #set1_test1.append(model_test1.score(in_set1, out_set1))\n",
    "#     #set2_test1.append(model_test1.score(in_set2, out_set2))\n",
    "# # Do the plotting\n",
    "# plt.figure(figsize=(15,10))\n",
    "# plt.subplot(121)\n",
    "# plt.semilogx(alpha_s, set1_test1, label='set1 score')\n",
    "# plt.semilogx(alpha_s, set2_test1, label='set2 score')\n",
    "# plt.ylabel('Score')\n",
    "# plt.xlabel('Parameter alpha')\n",
    "# plt.legend()\n",
    "\n",
    "# #Redo with set2 used as training data\n",
    "# set1_test2 = []\n",
    "# set2_test2 = []\n",
    "# for alpha in alpha_s:\n",
    "#     model_test2 = linear_model.Ridge(alpha=alpha)\n",
    "#     # Fit model based on *test* data (and alpha)\n",
    "#     model_test2.fit(in_set2, out_set2)\n",
    "#     #set1_test2.append(model_test2.score(in_set1, out_set1))\n",
    "#     #set2_test2.append(model_test2.score(in_set2, out_set2))\n",
    "#     set1_test2.append(metrics.r2_score(model_test2.predict(in_set1), out_set1))\n",
    "#     set2_test2.append(metrics.r2_score(model_test2.predict(in_set2), out_set2))\n",
    "# # Do the plotting\n",
    "# plt.subplot(122)\n",
    "# plt.semilogx(alpha_s, set1_test2, label='set1 score')\n",
    "# plt.semilogx(alpha_s, set2_test2, label='set2 score')\n",
    "# plt.ylabel('Score')\n",
    "# plt.xlabel('Parameter alpha')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# print([a_i - b_i for a_i, b_i in zip(set1_test2,set1_test1)])\n",
    "# print([a_i - b_i for a_i, b_i in zip(set2_test2,set2_test1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_CV=linear_model.RidgeCV(alphas=alpha_s[1:]).fit(in_set1, out_set1)\n",
    "\n",
    "# print(model_CV.score(in_set1, out_set1))\n",
    "# print(model_CV.score(in_set2, out_set2))\n",
    "# print( )\n",
    "# model_CV=linear_model.RidgeCV(alphas=alpha_s[1:]).fit(in_set2, out_set2)\n",
    "\n",
    "# print(model_CV.score(in_set1, out_set1))\n",
    "# print(model_CV.score(in_set2, out_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RF_hpc_clean]",
   "language": "python",
   "name": "conda-env-RF_hpc_clean-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
