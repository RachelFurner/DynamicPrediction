{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up three layer Lorenz 95 model as in D&B paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on version from https://en.wikipedia.org/wiki/Lorenz_96_model\n",
    "# and ammended to match D&B paper - not sure how to initialise, paper doesn't say, so have gone \n",
    "# with forcing values for x variables and random for y and z variables.\n",
    "\n",
    "\n",
    "I = 8\n",
    "J = 8\n",
    "K = 8\n",
    "\n",
    "F = 20  # forcing\n",
    "\n",
    "h = 1\n",
    "c = 10\n",
    "b = 10\n",
    "e = 10\n",
    "d = 10\n",
    "\n",
    "gz = 1\n",
    "\n",
    "\n",
    "#def Lorenz96(t, state):\n",
    "def Lorenz96(t, state):\n",
    "    # unpack input array\n",
    "    x=state[0:K]\n",
    "    y=state[K:J*K+K]\n",
    "    z=state[J*K+K:I*J*K+J*K+K]\n",
    "    y=y.reshape(J,K)\n",
    "    z=z.reshape(I,J,K)\n",
    "    \n",
    "    # compute state derivatives\n",
    "    dx = np.zeros((K))\n",
    "    dy = np.zeros((J,K))\n",
    "    dz = np.zeros((I,J,K))\n",
    "    # Do the X variable\n",
    "    # first the 3 edge cases: i=1,2,K-1\n",
    "    dx[0]   = x[K-1] * (x[1] - x[K-2]) - x[0]   + F - h*c/b * sum(y[:,0])\n",
    "    dx[1]   =   x[0] * (x[2] - x[K-1]) - x[1]   + F - h*c/b * sum(y[:,1])  \n",
    "    dx[K-1] = x[K-2] * (x[0] - x[K-3]) - x[K-1] + F - h*c/b * sum(y[:,K-1])  \n",
    "    # then the general case\n",
    "    for k in range(2, K-1):\n",
    "        dx[k] = x[k-1] * (x[k+1] - x[k-2]) - x[k] + F - h*c/b * sum(y[:,k])\n",
    "\n",
    "    # Do the Y variable\n",
    "    # first the 3 edge cases: i=1,2,K-1\n",
    "    for k in range(0,K):\n",
    "        dy[0,k]   = - c*b * y[1,k]   * ( y[2,k] - y[J-1,k] ) - c * y[0,k]    + h*c/b * x[k] - h*e/d * sum(z[:,0,k])\n",
    "        dy[J-2,k] = - c*b * y[J-1,k] * ( y[0,k] - y[J-3,k] ) - c * y[J-2,k]  + h*c/b * x[k] - h*e/d * sum(z[:,J-2,k])\n",
    "        dy[J-1,k] = - c*b * y[0,k]   * ( y[1,k] - y[J-2,k] ) - c * y[J-1,k]  + h*c/b * x[k] - h*e/d * sum(z[:,J-1,k])\n",
    "        # then the general case\n",
    "        for j in range(1, J-2):\n",
    "            dy[j,k] = - c*b * y[j+1,k] * ( y[j+2,k] - y[j-1,k] ) - c * y[j,k]  + h*c/b * x[k] - h*e/d * sum(z[:,j,k])\n",
    "\n",
    "    # Do the Z variable\n",
    "    # first the 3 edge cases: i=1,2,K-1\n",
    "    for k in range(0,K):\n",
    "        for j in range (0,J):\n",
    "            dz[0,j,k]   = e*d * z[I-1,j,k] * ( z[1,j,k] - z[I-2,j,k] ) - gz*e * z[0,j,k]   + h*e/d * y[j,k]\n",
    "            dz[1,j,k]   = e*d * z[0,j,k]   * ( z[2,j,k] - z[I-1,j,k] ) - gz*e * z[1,j,k]   + h*e/d * y[j,k]\n",
    "            dz[I-1,j,k] = e*d * z[I-2,j,k] * ( z[0,j,k] - z[I-3,j,k] ) - gz*e * z[I-1,j,k] + h*e/d * y[j,k]\n",
    "            # then the general case\n",
    "            for i in range(2,I-1):\n",
    "                dz[i,j,k] = e*d * z[i-1,j,k] * ( z[i+1,j,k] - z[i-2,j,k] ) - gz*e * z[i,j,k] + h*e/d * y[j,k]\n",
    "\n",
    "    # return the state derivatives\n",
    "    # reshape and cat into single array\n",
    "    d_state = np.concatenate((dx,dy.reshape(J*K,),dz.reshape(I*J*K,)))\n",
    "    \n",
    "    return d_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros((K)) # ??\n",
    "y0 = np.random.rand(J,K) # Random??\n",
    "z0 = np.random.rand(I,J,K) # Random??\n",
    "\n",
    "state0 = np.concatenate((x0,y0.reshape(J*K,),z0.reshape(I*J*K,)))\n",
    "\n",
    "int_t = 0.005\n",
    "#t_span = np.arange(0.0, 2000000.0, int_t)\n",
    "t_span = np.arange(0.0, 20.0, int_t)\n",
    "t_len=len(t_span)\n",
    "\n",
    "#state = solve_ivp(Lorenz96, t_span, state0, method='RK45')['y']\n",
    "state = odeint(Lorenz96, state0, t_span, tfirst=True)\n",
    "\n",
    "x=state[:,0:K]\n",
    "y=state[:,K:J*K+K].reshape(t_len,J,K)\n",
    "z=state[:,J*K+K:I*J*K+J*K+K].reshape(t_len,I,J,K)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot it\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# fig = plt.figure()\n",
    "# ax = fig.gca(projection='3d')\n",
    "# ax.plot(x[:,0],x[:,1],x[:,2])\n",
    "# ax.set_xlabel('$x_1$')\n",
    "# ax.set_ylabel('$x_2$')\n",
    "# ax.set_zlabel('$x_3$')\n",
    "# plt.show()\n",
    "\n",
    "# # plot first and second x values over time\n",
    "# fig = plt.figure(figsize=(20,4))\n",
    "# ax = plt.subplot(111)\n",
    "# ax.plot(x[:,0])\n",
    "# ax.plot(x[:,1])\n",
    "# ax.plot(x[:,3])\n",
    "# ax.plot(x[:,5])\n",
    "# ax.plot(x[:,7])\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up NN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two matching sequential NNs\n",
    "\n",
    "# D&B use two hidden layers with 20 neurons each\n",
    "\n",
    "H = 20 # no of nodes\n",
    "\n",
    "h1 = nn.Sequential(nn.Linear(4, H), nn.Tanh(), \n",
    "                   nn.Linear(H, H), nn.Tanh(), \n",
    "                   nn.Linear(H, H), nn.Tanh(), \n",
    "                   nn.Linear(H, 1))\n",
    "h2 = pickle.loads(pickle.dumps(h1))\n",
    "\n",
    "no_epochs=2\n",
    "# Need to normalise input and output data?!?!?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train to first order objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data in training - output pairs\n",
    "L95_data_1st = np.zeros((1,5))\n",
    "\n",
    "i=0\n",
    "for time in range(1, t_len, int(1/0.005)):\n",
    "    # xloc = 0 case\n",
    "    if i == 0:\n",
    "        L95_data_1st[i,:]=x[time-1, K-2], x[time-1,K-1], x[time-1,0], x[time-1,1], x[time, 1]\n",
    "        i=1\n",
    "    else:\n",
    "        L95_data_1st = np.vstack(( L95_data_1st, np.array([x[time-1, K-2], x[time-1,K-1], x[time-1,0],\n",
    "                                                   x[time-1,1], x[time, 1]], dtype=float) ))\n",
    "    # xloc = 1 case\n",
    "    L95_data_1st = np.vstack(( L95_data_1st, np.array([x[time-1, K-1], x[time-1,0], x[time-1,1], \n",
    "                                               x[time-1,2], x[time, 2]], dtype=float) ))\n",
    "    for xloc in range(2,K-1):\n",
    "        L95_data_1st = np.vstack(( L95_data_1st, np.array([x[time-1, xloc-2], x[time-1,xloc-1], x[time-1,xloc],\n",
    "                                                   x[time-1,xloc+1],x[time, xloc]], dtype=float) ))\n",
    "    # xloc = K-1 case\n",
    "    L95_data_1st = np.vstack(( L95_data_1st, np.array([x[time-1, K-3], x[time-1,K-2], x[time-1,K-1], \n",
    "                                               x[time-1,0], x[time, K-1]], dtype=float) ))\n",
    "    \n",
    "no_samples = L95_data_1st.shape[0]\n",
    "print('Train to first order objective')\n",
    "print('no samples : ', no_samples)\n",
    "\n",
    "L95_tensor_1st = torch.FloatTensor(L95_data_1st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt1 = torch.optim.Adagrad(h1.parameters(), lr=0.1)\n",
    "opt1 = torch.optim.SGD(h1.parameters(), lr=0.1)  # Stochastic gradient descent as optimiser\n",
    "# should I be setting a 'scheduler' and add a call to scheduler.step() ?\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(no_epochs):  # in D&B paper the NN's were trained for at least 200 epochs....\n",
    "    for i in range(no_samples):\n",
    "        opt1.zero_grad()\n",
    "        estimate = L95_tensor_1st[i,2] + h1(L95_tensor_1st[i,0:4])\n",
    "        loss = (estimate - L95_data_1st[i,4]).abs().mean()  # mean absolute error\n",
    "        loss.backward()\n",
    "        train_loss.append(loss.item())\n",
    "        opt1.step()\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.savefig('train_loss_1storderobjective.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train to second order objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data in training - output pairs\n",
    "L95_data_2nd = np.zeros((1,9))\n",
    "\n",
    "i=0\n",
    "for time in range(2, t_len, int(1/0.005)):\n",
    "    # xloc = 0 case\n",
    "    if i == 0:\n",
    "        L95_data_2nd[i,:] = x[time-2, K-2], x[time-2,K-1], x[time-2,0], x[time-2,1], \\\n",
    "                            x[time-1, K-2], x[time-1,K-1], x[time-1,0], x[time-1,1], \\\n",
    "                            x[time, 1]\n",
    "        i=1\n",
    "    else:\n",
    "        L95_data_2nd = np.vstack(( L95_data_2nd,\n",
    "                                   np.array([x[time-2, K-2], x[time-2,K-1], x[time-2,0], x[time-2,1],\n",
    "                                             x[time-1, K-2], x[time-1,K-1], x[time-1,0], x[time-1,1],\n",
    "                                             x[time, 1]], dtype=float) ))\n",
    "    # xloc = 1 case\n",
    "    L95_data_2nd = np.vstack(( L95_data_2nd,\n",
    "                               np.array([x[time-2, K-1], x[time-2,0], x[time-2,1], x[time-2,2],\n",
    "                                         x[time-1, K-1], x[time-1,0], x[time-1,1], x[time-1,2],\n",
    "                                         x[time, 2]], dtype=float) ))\n",
    "    for xloc in range(2,K-1):\n",
    "        L95_data_2nd = np.vstack(( L95_data_2nd,\n",
    "                                   np.array([x[time-2, xloc-2], x[time-2,xloc-1], x[time-2,xloc], x[time-2,xloc+1],\n",
    "                                             x[time-1, xloc-2], x[time-1,xloc-1], x[time-1,xloc], x[time-1,xloc+1],\n",
    "                                             x[time, xloc]], dtype=float) ))\n",
    "    # xloc = K-1 case\n",
    "    L95_data_2nd = np.vstack(( L95_data_2nd,\n",
    "                               np.array([x[time-2, K-3], x[time-2,K-2], x[time-2,K-1], x[time-2,0],\n",
    "                                         x[time-1, K-3], x[time-1,K-2], x[time-1,K-1], x[time-1,0],\n",
    "                                         x[time, K-1]], dtype=float) ))\n",
    "\n",
    "no_samples = L95_data_2nd.shape[0]\n",
    "print('Train to second order objective')\n",
    "print('no samples : ', no_samples)\n",
    "\n",
    "L95_tensor_2nd = torch.FloatTensor(L95_data_2nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt2 = torch.optim.Adagrad(h2.parameters(), lr=0.1)\n",
    "opt2 = torch.optim.SGD(h2.parameters(), lr=0.1)  # Stochastic gradient descent as optimiser\n",
    "# should I be setting a 'scheduler' and add a call to scheduler.step() ?\n",
    "\n",
    "train_loss2 = []\n",
    "for epoch in range(no_epochs):  # in D&B paper the NN's were trained for at least 200 epochs....\n",
    "    for i in range(no_samples):\n",
    "        opt2.zero_grad()\n",
    "        estimate = L95_tensor_2nd[i,6] + 0.5*( 3*h2(L95_tensor_2nd[i,4:8]) - h2(L95_tensor_2nd[i,0:4]) )\n",
    "        loss = (estimate - L95_data_2nd[i,8]).abs().mean()  # mean absolute error\n",
    "        loss.backward()\n",
    "        train_loss2.append(loss.item())\n",
    "        opt2.step()\n",
    "    \n",
    "plt.plot(train_loss2);\n",
    "plt.savefig('train_loss_2ndorderobjective.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at learned functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in simple integrators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_integrator(init, h, num_steps):\n",
    "    z = np.array([init])\n",
    "    for t in range(num_steps):\n",
    "        deltaz = np.zeros((K))\n",
    "        #xloc = 0 case\n",
    "        deltaz[0] = ( z[-1,0] + h(torch.FloatTensor([[[ z[-1, K-2], x[-1,K-1], x[-1,0], x[-1,1] ]]] )) ).item()\n",
    "        # xloc = 1 case\n",
    "        deltaz[1] = ( z[-1,1] + h(torch.FloatTensor([[[ z[-1, K-1], x[-1,0], x[-1,1], x[-1,2] ]]] )) ).item()\n",
    "        # xloc = 2 to K-2 cases\n",
    "        for xloc in range(2,K-1):\n",
    "            deltaz[xloc] = ( z[-1,xloc] + h(torch.FloatTensor([[[ z[-1, xloc-2], x[-1,xloc-1], x[-1,xloc], x[-1,xloc+1] ]]] )) ).item()\n",
    "        # xloc = K-1 case\n",
    "        deltaz[K-1] = ( z[-1,K-1] + h(torch.FloatTensor([[[ z[-1, K-3], x[-1,K-2], x[-1,K-1], x[-1,0] ]]] )) ).item()\n",
    "        z_next = z + deltaz\n",
    "        z=np.vstack((z, z_next))\n",
    "        \n",
    "    return np.array(z)\n",
    "\n",
    "train1test1 = first_order_integrator(x[0,:], h1, int(4/int_t))\n",
    "train2test1 = first_order_integrator(x[0,:], h2, int(4/int_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it..\n",
    "\n",
    "plt.plot(x[:int(4/int_t),4])\n",
    "plt.plot(train1test1[:,4])\n",
    "plt.plot(train2test1[:,4])\n",
    "plt.legend(['data', '1st', '2nd'])\n",
    "plt.title('first order integrator performance')\n",
    "plt.ylim(-30, 30)\n",
    "plt.show()\n",
    "plt.savefig('1storder_int_performance.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_integrator(init, h, num_steps):\n",
    "    z = first_order_integrator(init, h, 1)\n",
    "    for t in range(num_steps-1):\n",
    "        new = np.zeros((K))\n",
    "        # xloc = 0 case\n",
    "        new[0] = ( z[-1,0] + 0.5 * \n",
    "                 ( 3 * h(torch.FloatTensor([[[ z[-1, K-2], x[-1,K-1], x[-1,0], x[-1,1] ]]] ))\n",
    "                     - h(torch.FloatTensor([[[ z[-2, K-2], x[-2,K-1], x[-2,0], x[-2,1] ]]] )) ).item() )\n",
    "        # xloc = 1 case\n",
    "        new[1] = ( z[-1,1] + 0.5 * \n",
    "                 ( 3 * h(torch.FloatTensor([[[ z[-1, K-1], x[-1,0], x[-1,1], x[-1,2] ]]] ))\n",
    "                     - h(torch.FloatTensor([[[ z[-2, K-1], x[-2,0], x[-2,1], x[-2,2] ]]] )) ).item() )\n",
    "        # xloc = 2 to K-2 cases\n",
    "        for xloc in range(2,K-1):\n",
    "            new[xloc] = ( z[-1,xloc] + 0.5 *\n",
    "                        ( 3 * h(torch.FloatTensor([[[ z[-1, xloc-2], x[-1,xloc-1], x[-1,xloc], x[-1,xloc+1] ]]] ))\n",
    "                            - h(torch.FloatTensor([[[ z[-2, xloc-2], x[-2,xloc-1], x[-2,xloc], x[-2,xloc+1] ]]] )) ).item() )\n",
    "        # xloc = K-1 case\n",
    "        new[K-1] = ( z[-1,K-1] + 0.5 *\n",
    "                   ( 3 * h(torch.FloatTensor([[[ z[-1, K-3], x[-1,K-2], x[-1,K-1], x[-1,0] ]]] )) \n",
    "                       - h(torch.FloatTensor([[[ z[-2, K-3], x[-2,K-2], x[-2,K-1], x[-2,0] ]]] )) ).item() )\n",
    "         \n",
    "        z=np.vstack((z, new))\n",
    "        \n",
    "    return np.array(z)\n",
    "\n",
    "train1test2 = second_order_integrator(x[0,:], h1, 4/int_t)\n",
    "train2test2 = second_order_integrator(x[0,:], h2, 4/int_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it..\n",
    "\n",
    "plt.plot(x[:4/int_t,4])\n",
    "plt.ylim(-30,30)\n",
    "plt.plot(train1test2[:,4])\n",
    "plt.plot(train2test2[:,4])\n",
    "plt.legend(['data', '1st', '2nd']);\n",
    "plt.title('second order integrator performance');\n",
    "plt.ylim(-30, 30)\n",
    "plt.show()\n",
    "plt.savefig('1storder_int_performance.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_sequence = np.array([1,2,6,3,10,5,6], dtype=float)\n",
    "\n",
    "# data_simple = torch.FloatTensor(np.stack((toy_sequence[:-1], toy_sequence[1:])).T)\n",
    "# data_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# H = 20\n",
    "\n",
    "# h1 = nn.Sequential(nn.Linear(1, H), nn.ReLU(), \n",
    "# #                    nn.Linear(H, H), nn.ReLU(), \n",
    "#                    nn.Linear(H, H), nn.ReLU(), \n",
    "#                    nn.Linear(H, 1))\n",
    "# h2 = pickle.loads(pickle.dumps(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain = np.linspace(-10, 10)\n",
    "\n",
    "# out = h1(torch.FloatTensor(domain)[:,None]).detach().numpy()\n",
    "# plt.plot(domain, out)\n",
    "# out = h2(torch.FloatTensor(domain)[:,None]).detach().numpy()\n",
    "# plt.plot(domain, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training to first-order objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt1 = torch.optim.Adagrad(h1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_simple)\n",
    "# # data_simple[:,0][:,None] + h1(data_simple[:,0][:,None]), \n",
    "# print(data_simple[:,0])\n",
    "# print(data_simple[:,0][:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss = []\n",
    "# for i in range(20000):\n",
    "#     opt1.zero_grad()\n",
    "#     estimate = data_simple[:,0][:,None] + h1(data_simple[:,0][:,None])\n",
    "#     loss = (estimate - data_simple[:,1][:,None]).abs().mean()\n",
    "#     loss.backward()\n",
    "#     train_loss.append(loss.item())\n",
    "#     opt1.step()\n",
    "    \n",
    "# plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_loss[200:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training to second-order objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt2 = torch.optim.Adagrad(h2.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.FloatTensor(np.stack((toy_sequence[:-2], toy_sequence[1:-1], toy_sequence[2:])).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss2 = []\n",
    "\n",
    "# for i in range(20000):\n",
    "#     opt2.zero_grad()\n",
    "#     estimate = data[:,1][:,None] + 0.5*(3*h2(data[:,1][:,None]) - h2(data[:,0][:,None]))\n",
    "#     loss = (estimate - data[:,2][:,None]).abs().mean()\n",
    "#     loss.backward()\n",
    "#     train_loss2.append(loss.item())\n",
    "#     opt2.step()\n",
    "    \n",
    "# plt.plot(train_loss2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the learned functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain = np.linspace(-3, 3)\n",
    "\n",
    "# out = h1(torch.FloatTensor(domain)[:,None]).detach().numpy()\n",
    "# plt.plot(domain, out)\n",
    "# out = h2(torch.FloatTensor(domain)[:,None]).detach().numpy()\n",
    "# plt.plot(domain, out)\n",
    "# plt.legend(['1st order', '2nd order']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test use of each in \"simple\" integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_integrator(init, h, num_steps):\n",
    "#     z = [init]\n",
    "#     for t in range(num_steps):\n",
    "#         z.append((z[-1] + h(torch.FloatTensor([[z[-1]]]))).item())\n",
    "#     return np.array(z)\n",
    "\n",
    "\n",
    "# train1test1 = simple_integrator(toy_sequence[0], h1, len(toy_sequence)-1)\n",
    "# train2test1 = simple_integrator(toy_sequence[0], h2, len(toy_sequence)-1)\n",
    "\n",
    "# plt.plot(toy_sequence)\n",
    "# plt.plot(train1test1)\n",
    "# plt.plot(train2test1)\n",
    "# #plt.ylim(ylim)\n",
    "# # plt.xlim(700, 1000)\n",
    "# plt.legend(['data', '1st', '2nd']);\n",
    "# plt.title('first order integrator performance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def second_order_integrator(init, h, num_steps):\n",
    "#     z = simple_integrator(init, h, 1).tolist()\n",
    "#     for t in range(num_steps-1):\n",
    "#         z.append((z[-1] + 0.5*(3*h(torch.FloatTensor([[z[-1]]])) - h(torch.FloatTensor([[z[-2]]])))).item())\n",
    "#     return np.array(z)\n",
    "\n",
    "# train1test2 = second_order_integrator(toy_sequence[0], h1, len(toy_sequence)-1)\n",
    "# train2test2 = second_order_integrator(toy_sequence[0], h2, len(toy_sequence)-1)\n",
    "\n",
    "# plt.plot(toy_sequence)\n",
    "# #ylim = np.array(plt.ylim())*1.5\n",
    "# plt.plot(train1test2)\n",
    "# plt.plot(train2test2)\n",
    "# #plt.ylim(ylim);\n",
    "# # plt.xlim(700, 1000)\n",
    "# plt.legend(['data', '1st', '2nd']);\n",
    "# plt.title('second order integrator performance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
